{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PostgreSQL Hacker Helper This is a VS Code extension to assist PostgreSQL Hackers - source code developers. Table of contents Configuration PostgreSQL setup Development scripts VS Code setup Tutorials: Create extension","title":"Home"},{"location":"#postgresql-hacker-helper","text":"This is a VS Code extension to assist PostgreSQL Hackers - source code developers.","title":"PostgreSQL Hacker Helper"},{"location":"#table-of-contents","text":"Configuration PostgreSQL setup Development scripts VS Code setup Tutorials: Create extension","title":"Table of contents"},{"location":"configuration/","text":"Configuration Extension has multiple settings to customize different aspects. VS Code settings There are 4 settings: postgresql-hacker-helper.logLevel - minimum log level (for old VS Code up to 1.74.0). Minimum level of log messages. By default - INFO . If using VS Code 1.74.0 ang greater use Output channel logger settings. postgresql-hacker-helper.srcPath - Path to source code directory Relative path to custom PostgreSQL source code directory. Use it, if source code files are not in your workspace root (i.e. in ${workspaceFolder}/postgresql ). Used for searching files (node tag files, pg_bsd_indent and so on). If not specified search starts from workspace root. postgresql-hacker-helper.pg_bsd_indentPath - Path to pg_bsd_indent Path to pg_bsd_indent tool. Required for formatting support. Use it if you have pg_bsd_indent installed globally or want to use specific version. If not specified, it will be searched in *srcPath*/src/tools directory. If specified, and failed to run extension will try to build it. postgresql-hacker-helper.maxContainerLength - max length of elements shown in container types: List , arrays, Bitmapset , hash tables, etc... How many elements must be shown in elements of container type. This setting must prevent using garbage stored in fields. Default: 128 Configuration file Extension has config file with custom settings - .vscode/pgsql_hacker_helper.json . You can create file manually or using command PgSQL: Open or create configuration file . Json schema will assist you while editing. Extension tracks changes in the file and rereads it, when necessary. Also, you can run PgSQL: Refresh configuration file command. NOTE: after debug session have started changes in configuration file will not be reflected. Arrays { \"arrays\": [ { // struct name without any qualifiers (const, volatile, etc...) \"typeName\": \"string\", // name of member in that struct, storing array \"memberName\": \"string\", // expression to evaluate to get length \"lengthExpression\": \"string\" } ] } In PostgreSQL arrays can be stored in List * variables or as simple C-arrays: struct member storing pointer and another member storing it's length. For given struct... struct Sample { /* Array */ int *array; /* Length of array */ int size; } ...we have next configuration entry: { \"arrays\": [ { \"typeName\": \"Sample\", \"memberName\": \"array\", \"lengthExpression\": \"size\" } ] } Length expression can be in 2 forms: Member name concatenated to parent object. In such case lengthExpression is just concatenated to parent object as parent->lengthExpression . As it is concatenated, then you can add some other expressions to it. Example above: size will be evaluated as ((Sample *)0xFFFF)->size Generic expression (starts with ! ) lengthExpression represents arbitrary expression which must be evaluated to some number (integer). This expression starts with ! to distinguish between this form and member name form. Example above: !{}->size will be evaluated as ((ParentType)0xFFFF)->size NOTES: You can refer to parent object using {} , i.e. !{}->member1 + {}->member2 or the same in member form member1 + {}->member2 . Expression can contain any other entries, i.e. for PlannerInfo->simple_rel_array expression is simple_rel_array_size + 1 . Aliases ( typedef ) { \"aliases\": [ { // Name of alias \"alias\": \"string\", // Actual type \"type\": \"string\" } ] } There are many typedef s in code and some of them may be for Node types. But, when resolving type, extension can not know that it is actually a typedef, so treat variable as simple, not Node derived. For such cases \"aliases\" field exists. It is array which defines aliases for different Node types - when we can not find suitable NodeTag for type we search alias and substitute type. Example: typedef Bitmapset *Relids - there is no T_Relids in the code, but T_Bitmapset exist. For it you can use this entry: { \"aliases\": [ { \"alias\": \"Relids\", \"type\": \"Bitmapset *\" } ] } Custom List * pointer types { \"customListTypes\": [ { // Struct or function name containing this 'List *' variable \"parent\": \"string\", // Name of member/variable inside \"parent\" \"member\": \"string\", // Actual pointer type \"type\": \"string\" } ] } Usually, List * contains Node types, but actually it can contain any pointer. Extension treats all List as they contain Node variables, but some lists contain non-Node types. You can specify your own custom types using customListTypes member. For this code... typedef struct SampleData { int value; } SampleData; typedef struct Sample { // Contains SampleData List *data; } Sample; void do_work() { List *list; SampleData *data; data = palloc(sizeof(SampleData)); data->value = 1; list = list_make1(data); /* ... */ } ...you can define this configuration: { \"customListTypes\": [ { \"type\": \"SampleData *\", // Member of struct \"parent\": \"Sample\", \"member\": \"data\" }, { \"type\": \"SampleData *\", // Variable inside function \"parent\": \"create_sample\", \"member\": \"list\" } ] } As you can mention, configuration is generalized, because it's clear from context how to handle parent HashTable entries HTAB { \"htab\": [ { // Struct or function name containing this HTAB \"parent\": \"string\", // Member/variable name inside parent \"member\": \"string\", // Stored type \"type\": \"string\" } ] } HTAB * Hash Table entries can be showed using hash_seq_search , but it returns void * - no information about it's type. Extension has built-in types for some HTAB s. For the following code... typedef struct SampleData { int value; } SampleData; typedef struct Sample { HTAB *data; } Sample; void do_work() { HTAB *htab = create_htab(); Sample *sample = palloc(sizeof(Sample)); sample->data = htab; /* ... */ } ...you can define next configuration: { \"htab\": [ { \"parent\": \"Sample\", \"member\": \"data\", \"type\": \"SampleData *\" }, { \"parent\": \"do_work\", \"member\": \"htab\", \"type\": \"SampleData *\" } ] } You can notice that configuration entry schema is the same as for custom List * type. _hash - simplehash { \"simplehash\": [ { // Prefix as you defined using SH_PREFIX \"prefix\": \"string\", // Stored type \"type\": \"string\" } ] } Also, there is support for lib/simplehash.h hash tables (\"simplehash\" further). They are code generated using macros, so for each specific hash table there are functions and structures defined. For the following code... typedef struct SimpleHashEntry { int value; } SimpleHashEntry; #define SH_PREFIX custom_prefix #define SH_ELEMENT_TYPE SimpleHashEntry #include \"lib/simplehash.h\" ...define next configuration: { \"simplehash\": [ { \"prefix\": \"custom_prefix\", \"type\": \"SimpleHashEntry *\" } ] } Identifiers of structures and functions are derived from prefix and generated the same way, i.e. PREFIX_iterator - structure-state for iterator. NOTE: compiler can apply unused symbol stripping, so after compilation there can be no structures/functions for iteration. In such situation, you should add some code that uses PREFIX_iterator , PREFIX_start_iterate and PREFIX_iterate (i.e. wrap such code with debug macros). Integer enum fields { \"enums\": [ { // Name of struct \"type\": \"string\", // Member of struct containing enum \"member\": \"string\", // Enum values stored in field: pair of macro name and declared value \"flags\": [ [\"Mask (macro)\", \"Mask (integer)\"], ], // Fields stored in field, values for which is got using bitmask \"fields\": [ [\"Field name\", \"Mask (macro)\", \"Mask (integer)\"] ] } ] } Some types may work with enums as plain uint32 (not enum ) and members of enum are defined using preprocessor's #define . For such types you can specify your own enum bitmask members. For the following code... typedef struct ParentType { int enum_member; } ParentType; /* Enum values */ #define EM_NOTHING 0x10 #define EM_SINGLE 0x20 #define EM_MULTIPLE 0x40 /* Mask to get length */ #define EM_LENGTH_MASK 0xF void some_function(ParentType *parent) { if (parent->enum_member & EM_MULTIPLE) { int length = parent->enum_member & EM_LENGTH_MASK; } } ...you can use configuration: { \"enums\": [ { \"type\": \"ParentType\", \"member\": \"enum_member\", \"flags\": [ [\"EM_NOTHING\", \"0x10\"], [\"EM_SINGLE\", \"0x20\"], [\"EM_MULTIPLE\", \"0x40\"], ], \"fields\": [ [\"length\", \"EM_LENGTH_MASK\", \"0xF\"] ] } ] } NOTE: macro definitions are added to debug symbols only when using -g3 level during compilation, otherwise debugger can not use macro names. If debugger can not use macros it will switch to numeric values - that because numeric values are required. NodeTags { // Array of custom NodeTag values \"nodetags\": [ \"string\" ] } NodeTag values are required to find Node types. Extension ships with set of builtin tags, but they can be outdated or you are created new Node type. If so, just add them to this list. If you specify type with T_ prefix - it will be trimmed. Also, when debug session starts, extension will parse nodetags.h file to find new NodeTags. If it will find some, then extension will automatically add them to this list. Custom typedefs.list { \"typedefs\": [ \"/path/to/typedefs.list\" ] } For formatting src/tools/pgindent is used. It requires typedefs.list file for correct work - one lies inside directory itself, but when you are developing extension you may have your own copy for extension's types. typedefs setting contains list of typedefs.list files - each string is a path which can be in 2 forms: Absolute - specified file is used Relative - file with base folder as postgresql-hacker-helper.srcPath is used Example: { \"typedefs\": [ \"contrib/pgext1/first.typedefs.list\", \"contrib/pgext2/second.typedefs.list\" ] } For convenience, if you will try to format file in contrib's directory, extension will try to detect typedefs.list in it without specifying it explicitly in configuration file. I.e. if you are formatting file contrib/my_ext/my_ext.c , then extension will probe contrib/my_ext/typedefs.list . There is handy command PgSQL: Find custom typedefs.list in repository that will execute shell command to find all *typedefs.list files in repository.","title":"Configuration"},{"location":"configuration/#configuration","text":"Extension has multiple settings to customize different aspects.","title":"Configuration"},{"location":"configuration/#vs-code-settings","text":"There are 4 settings: postgresql-hacker-helper.logLevel - minimum log level (for old VS Code up to 1.74.0). Minimum level of log messages. By default - INFO . If using VS Code 1.74.0 ang greater use Output channel logger settings. postgresql-hacker-helper.srcPath - Path to source code directory Relative path to custom PostgreSQL source code directory. Use it, if source code files are not in your workspace root (i.e. in ${workspaceFolder}/postgresql ). Used for searching files (node tag files, pg_bsd_indent and so on). If not specified search starts from workspace root. postgresql-hacker-helper.pg_bsd_indentPath - Path to pg_bsd_indent Path to pg_bsd_indent tool. Required for formatting support. Use it if you have pg_bsd_indent installed globally or want to use specific version. If not specified, it will be searched in *srcPath*/src/tools directory. If specified, and failed to run extension will try to build it. postgresql-hacker-helper.maxContainerLength - max length of elements shown in container types: List , arrays, Bitmapset , hash tables, etc... How many elements must be shown in elements of container type. This setting must prevent using garbage stored in fields. Default: 128","title":"VS Code settings"},{"location":"configuration/#configuration-file","text":"Extension has config file with custom settings - .vscode/pgsql_hacker_helper.json . You can create file manually or using command PgSQL: Open or create configuration file . Json schema will assist you while editing. Extension tracks changes in the file and rereads it, when necessary. Also, you can run PgSQL: Refresh configuration file command. NOTE: after debug session have started changes in configuration file will not be reflected.","title":"Configuration file"},{"location":"configuration/#arrays","text":"{ \"arrays\": [ { // struct name without any qualifiers (const, volatile, etc...) \"typeName\": \"string\", // name of member in that struct, storing array \"memberName\": \"string\", // expression to evaluate to get length \"lengthExpression\": \"string\" } ] } In PostgreSQL arrays can be stored in List * variables or as simple C-arrays: struct member storing pointer and another member storing it's length. For given struct... struct Sample { /* Array */ int *array; /* Length of array */ int size; } ...we have next configuration entry: { \"arrays\": [ { \"typeName\": \"Sample\", \"memberName\": \"array\", \"lengthExpression\": \"size\" } ] } Length expression can be in 2 forms: Member name concatenated to parent object. In such case lengthExpression is just concatenated to parent object as parent->lengthExpression . As it is concatenated, then you can add some other expressions to it. Example above: size will be evaluated as ((Sample *)0xFFFF)->size Generic expression (starts with ! ) lengthExpression represents arbitrary expression which must be evaluated to some number (integer). This expression starts with ! to distinguish between this form and member name form. Example above: !{}->size will be evaluated as ((ParentType)0xFFFF)->size NOTES: You can refer to parent object using {} , i.e. !{}->member1 + {}->member2 or the same in member form member1 + {}->member2 . Expression can contain any other entries, i.e. for PlannerInfo->simple_rel_array expression is simple_rel_array_size + 1 .","title":"Arrays"},{"location":"configuration/#aliases-typedef","text":"{ \"aliases\": [ { // Name of alias \"alias\": \"string\", // Actual type \"type\": \"string\" } ] } There are many typedef s in code and some of them may be for Node types. But, when resolving type, extension can not know that it is actually a typedef, so treat variable as simple, not Node derived. For such cases \"aliases\" field exists. It is array which defines aliases for different Node types - when we can not find suitable NodeTag for type we search alias and substitute type. Example: typedef Bitmapset *Relids - there is no T_Relids in the code, but T_Bitmapset exist. For it you can use this entry: { \"aliases\": [ { \"alias\": \"Relids\", \"type\": \"Bitmapset *\" } ] }","title":"Aliases (typedef)"},{"location":"configuration/#custom-list-pointer-types","text":"{ \"customListTypes\": [ { // Struct or function name containing this 'List *' variable \"parent\": \"string\", // Name of member/variable inside \"parent\" \"member\": \"string\", // Actual pointer type \"type\": \"string\" } ] } Usually, List * contains Node types, but actually it can contain any pointer. Extension treats all List as they contain Node variables, but some lists contain non-Node types. You can specify your own custom types using customListTypes member. For this code... typedef struct SampleData { int value; } SampleData; typedef struct Sample { // Contains SampleData List *data; } Sample; void do_work() { List *list; SampleData *data; data = palloc(sizeof(SampleData)); data->value = 1; list = list_make1(data); /* ... */ } ...you can define this configuration: { \"customListTypes\": [ { \"type\": \"SampleData *\", // Member of struct \"parent\": \"Sample\", \"member\": \"data\" }, { \"type\": \"SampleData *\", // Variable inside function \"parent\": \"create_sample\", \"member\": \"list\" } ] } As you can mention, configuration is generalized, because it's clear from context how to handle parent","title":"Custom List * pointer types"},{"location":"configuration/#hashtable-entries","text":"","title":"HashTable entries"},{"location":"configuration/#htab","text":"{ \"htab\": [ { // Struct or function name containing this HTAB \"parent\": \"string\", // Member/variable name inside parent \"member\": \"string\", // Stored type \"type\": \"string\" } ] } HTAB * Hash Table entries can be showed using hash_seq_search , but it returns void * - no information about it's type. Extension has built-in types for some HTAB s. For the following code... typedef struct SampleData { int value; } SampleData; typedef struct Sample { HTAB *data; } Sample; void do_work() { HTAB *htab = create_htab(); Sample *sample = palloc(sizeof(Sample)); sample->data = htab; /* ... */ } ...you can define next configuration: { \"htab\": [ { \"parent\": \"Sample\", \"member\": \"data\", \"type\": \"SampleData *\" }, { \"parent\": \"do_work\", \"member\": \"htab\", \"type\": \"SampleData *\" } ] } You can notice that configuration entry schema is the same as for custom List * type.","title":"HTAB"},{"location":"configuration/#_hash-simplehash","text":"{ \"simplehash\": [ { // Prefix as you defined using SH_PREFIX \"prefix\": \"string\", // Stored type \"type\": \"string\" } ] } Also, there is support for lib/simplehash.h hash tables (\"simplehash\" further). They are code generated using macros, so for each specific hash table there are functions and structures defined. For the following code... typedef struct SimpleHashEntry { int value; } SimpleHashEntry; #define SH_PREFIX custom_prefix #define SH_ELEMENT_TYPE SimpleHashEntry #include \"lib/simplehash.h\" ...define next configuration: { \"simplehash\": [ { \"prefix\": \"custom_prefix\", \"type\": \"SimpleHashEntry *\" } ] } Identifiers of structures and functions are derived from prefix and generated the same way, i.e. PREFIX_iterator - structure-state for iterator. NOTE: compiler can apply unused symbol stripping, so after compilation there can be no structures/functions for iteration. In such situation, you should add some code that uses PREFIX_iterator , PREFIX_start_iterate and PREFIX_iterate (i.e. wrap such code with debug macros).","title":"_hash - simplehash"},{"location":"configuration/#integer-enum-fields","text":"{ \"enums\": [ { // Name of struct \"type\": \"string\", // Member of struct containing enum \"member\": \"string\", // Enum values stored in field: pair of macro name and declared value \"flags\": [ [\"Mask (macro)\", \"Mask (integer)\"], ], // Fields stored in field, values for which is got using bitmask \"fields\": [ [\"Field name\", \"Mask (macro)\", \"Mask (integer)\"] ] } ] } Some types may work with enums as plain uint32 (not enum ) and members of enum are defined using preprocessor's #define . For such types you can specify your own enum bitmask members. For the following code... typedef struct ParentType { int enum_member; } ParentType; /* Enum values */ #define EM_NOTHING 0x10 #define EM_SINGLE 0x20 #define EM_MULTIPLE 0x40 /* Mask to get length */ #define EM_LENGTH_MASK 0xF void some_function(ParentType *parent) { if (parent->enum_member & EM_MULTIPLE) { int length = parent->enum_member & EM_LENGTH_MASK; } } ...you can use configuration: { \"enums\": [ { \"type\": \"ParentType\", \"member\": \"enum_member\", \"flags\": [ [\"EM_NOTHING\", \"0x10\"], [\"EM_SINGLE\", \"0x20\"], [\"EM_MULTIPLE\", \"0x40\"], ], \"fields\": [ [\"length\", \"EM_LENGTH_MASK\", \"0xF\"] ] } ] } NOTE: macro definitions are added to debug symbols only when using -g3 level during compilation, otherwise debugger can not use macro names. If debugger can not use macros it will switch to numeric values - that because numeric values are required.","title":"Integer enum fields"},{"location":"configuration/#nodetags","text":"{ // Array of custom NodeTag values \"nodetags\": [ \"string\" ] } NodeTag values are required to find Node types. Extension ships with set of builtin tags, but they can be outdated or you are created new Node type. If so, just add them to this list. If you specify type with T_ prefix - it will be trimmed. Also, when debug session starts, extension will parse nodetags.h file to find new NodeTags. If it will find some, then extension will automatically add them to this list.","title":"NodeTags"},{"location":"configuration/#custom-typedefslist","text":"{ \"typedefs\": [ \"/path/to/typedefs.list\" ] } For formatting src/tools/pgindent is used. It requires typedefs.list file for correct work - one lies inside directory itself, but when you are developing extension you may have your own copy for extension's types. typedefs setting contains list of typedefs.list files - each string is a path which can be in 2 forms: Absolute - specified file is used Relative - file with base folder as postgresql-hacker-helper.srcPath is used Example: { \"typedefs\": [ \"contrib/pgext1/first.typedefs.list\", \"contrib/pgext2/second.typedefs.list\" ] } For convenience, if you will try to format file in contrib's directory, extension will try to detect typedefs.list in it without specifying it explicitly in configuration file. I.e. if you are formatting file contrib/my_ext/my_ext.c , then extension will probe contrib/my_ext/typedefs.list . There is handy command PgSQL: Find custom typedefs.list in repository that will execute shell command to find all *typedefs.list files in repository.","title":"Custom typedefs.list"},{"location":"create_extension/","text":"Tutorial: creating extension In this tutorial we will create extension ban_sus_query . It will check that DML queries contain predicates, otherwise will just throw an error. Next, in order not to mislead up, I will use term contrib for PostgreSQL extension, and for extension for PostgreSQL Hacker Helper VS Code extension. Creating initial files PostgreSQL has infrastructure for contrib building and installation. In short, contribs have a template architecture - most parts are common for all. So, for faster contrib creation we will use command: PgSQL: Bootstrap extension . It will prompt us to bootstrap some files - choose only C sources. After that we will have our contrib files created: Initial code Query execution pipeline has 3 stages: Parse/Semantic analysis - query string parsing and resolving tables Plan - query optimization and creating execution plan Execution - actual query execution Our logic will be added to the 2 stage, because we must check real execution plan, not Query. This is because after multiple transformations query can be changed in multiple ways - predicates can be deleted or added, therefore we may get a completely different query than in the original query string. To implement that we will create hook on planner - planner_hook . Inside we will invoke actual planner and check it's output for the existence of predicates. Starter code is the following: #include \"postgres.h\" #include \"fmgr.h\" #include \"optimizer/planner.h\" #ifdef PG_MODULE_MAGIC PG_MODULE_MAGIC; #endif static planner_hook_type prev_planner_hook; void _PG_init(void); void _PG_fini(void); static bool is_sus_query(Plan *plan) { /* ... */ return false; } static PlannedStmt * ban_sus_query_planner_hook(Query *parse, const char *query_string, int cursorOptions, ParamListInfo boundParams) { PlannedStmt *stmt; if (prev_planner_hook) stmt = prev_planner_hook(parse, query_string, cursorOptions, boundParams); else stmt = standard_planner(parse, query_string, cursorOptions, boundParams); if (is_sus_query(stmt->planTree)) ereport(ERROR, (errmsg(\"DML query does not contain predicates\"))); return stmt; } void _PG_init(void) { prev_planner_hook = planner_hook; planner_hook = ban_sus_query_planner_hook; } void _PG_fini(void) { planner_hook = prev_planner_hook; } Now we are ready to add \"business-logic\", but before let's understand how such suspicious queries look like. Examine queries Suspicious query - is a DELETE/UPDATE query that does not contain predicates. One of the benefits that we are checking already planned statements is that all predicates are already optimized in a sense that boolean rules are applied. Query plan - is a tree of Plan nodes. Each Plan contains lefttree / righttree - left and right children and qual - list of predicates to apply at this node. But we must check only UPDATE/DELETE nodes, not each node, - nodes for them is ModifyTable . Thus our goal is: traverse query tree, find ModifyTable and check that it's qual is not empty But, before run sample queries to look what their queries looks like (inside) and which predicates they have. For tests we will use this setup: -- Schema CREATE TABLE tbl(x int); -- Test queries DELETE FROM tbl; DELETE FROM tbl WHERE x = 0; UPDATE tbl SET x = 1; UPDATE tbl SET x = 1 WHERE x = 0; To do this we will use our contrib - install it using make install , add to shared_preload_libraries='ban_sus_query' and put a breakpoint to return in ban_sus_query_planner_hook function. When we run first DELETE query without predicate, we will see the following: PlannedStmt contains top-level ModifyTable with empty qual list Inner SeqScan also contains empty qual list Now run DELETE query with predicate: PlannedStmt still contains empty qual list Inner SeqScan now contains qual with single element - equality predicate This is no surprise, because our ModifyTable does not apply any filtering - it just takes tuples from children (note, that by convention single-child nodes store them in lefttree ), so it's qual is empty, but filtering is applied to SeqScan - we must check this. As you can mention, extension shows all Node variables with actual types, without showing generic Plan entry. Also extension is able to show you elements of container types ( List * in this example). More than that, it renders Expr nodes (expressions) as it was in a query, so you do not have to manually check each field, trying to figure out what expression it is. In vanilla PostgreSQL you would have to evaluate 2 expressions: (first) get NodeTag and (second) cast variable to obtained NodeTag . In this example, to show stmt->planTree all you need to do is expand the tree node in variables explorer, but manually (without extension), you need to evaluate (i.e. in watch ) 2 expressions/steps: ((Node *)planTree)->type get T_ModifyTable - tag of ModifyTable node, and then (ModifyTable *)planTree - show variable with real type. Such manipulations take roughly 5 second, but, as this time accumulates, totally it can take up to 1 hour in a day - just to show variable's contents! But there is not such support for Expr variables - you will not see their representation. For this you have to dump variable to log using pprint function, which is not very convenient when you developing in IDE. Now we are ready to write some code. is_sus_query implementation I repeat, our goal is to traverse query tree, find ModifyTable and check that it's qual is not empty , but now we can refine it: Search for ModifyTable in Plan tree and check that it's children have non-empty qual list As tree traversal is a recursive function, we will use 2 recursive functions: is_sus_query - main function that traverses plan tree to find ModifyTable node, and when it finds one invokes... contains_predicates - function that checks that this Plan node contains any predicate in a query Let's start with is_sus_query . All we have to do here is to check that Plan is a ModifyTable and if so, then check that it's children contain predicates. Node type checking is a frequent operation, so extension ships with some snippets - one of them is a isaif , which expands to if(IsA()) check: When we have determined, that it is a DML operation check that it is DELETE or UPDATE, because ModifyTable is used for other operations, i.e. INSERT . This is not hard - just check operation member. static bool is_sus_query(Plan *plan) { /* ... */ ModifyTable *modify = (ModifyTable *)plan; switch (modify->operation) { case CMD_UPDATE: case CMD_DELETE: /* Check predicates */ break; default: break; } /* ... */ } And now check these operations contain predicates using contains_predicates function (will be defined further). Also, do not forget to handle recursion: call is_sus_query for children and handle end case ( NULL ). The result function looks like this: static bool is_sus_query(Plan *plan) { /* Recursion end */ if (plan == NULL) return false; if (IsA(plan, ModifyTable)) { ModifyTable *modify = (ModifyTable *)plan; switch (modify->operation) { case CMD_UPDATE: case CMD_DELETE: return !contains_predicates(modify->plan.lefttree); default: break; } } /* Handle recursion */ return is_sus_query(plan->lefttree) || is_sus_query(plan->righttree); } contains_predicates implementation Now perform actual checking of the predicates existence using contains_predicates . Inside this function we must check that given Plan contains predicates. But situation is complicated by the fact that only base Plan is given and we do not know actual query. For example this query: DELETE FROM t1 using t2 where t1.x = t2.x; Will contain JOIN in lefttree of ModifyTable : Thus we have to clarify what does contains_predicates must check. In order not to complicate things a lot, we will just find first node with any predicate. static bool contains_predicates(Plan *plan) { if (plan == NULL) return false; if (plan->qual != NIL) return true; return contains_predicates(plan->lefttree) || contains_predicates(plan->righttree); } Testing First things first - test on example queries we defined above: postgres=# delete from tbl; ERROR: DML query does not contain predicates postgres=# delete from t1 where x = 0; DELETE 0 postgres=# update tbl set x = 0; ERROR: DML query does not contain predicates postgres=# update tbl set x = 0 where x = 0; UPDATE 0 It's working as expected. Also, as we injected our contrib as the last step, we can handle more complicated cases, like: postgres=# delete from t1 where true; ERROR: DML query does not contain predicates Further improvements This is just the beginning of the contrib, because there are lot's of corner cases that are not handled. For example, if we change true to false in last query, then we still will get an ERROR . That is because the database has realized that subquery will not return anything, so replaced with \"dummy\" Plan - Result node with FALSE one-time check, so nothing will be returned: Result So far we have seen how you can quickly create new contrib using single command that will create all necessary files. To write some templated code, we used isaif snippet to quickly add check for Node type. Also, we have traversed query plan tree and saw it's nodes, without requirement to obtain NodeTag and cast to given type, which incredibly boosts performance. And like the icing on the cake we saw expression representations of predicates. For our purposes this is not a very big deal, because query contained only 1 predicate, but in large queries with dozens of different predicates it's just a lifesaver.","title":"Create extension"},{"location":"create_extension/#tutorial-creating-extension","text":"In this tutorial we will create extension ban_sus_query . It will check that DML queries contain predicates, otherwise will just throw an error. Next, in order not to mislead up, I will use term contrib for PostgreSQL extension, and for extension for PostgreSQL Hacker Helper VS Code extension.","title":"Tutorial: creating extension"},{"location":"create_extension/#creating-initial-files","text":"PostgreSQL has infrastructure for contrib building and installation. In short, contribs have a template architecture - most parts are common for all. So, for faster contrib creation we will use command: PgSQL: Bootstrap extension . It will prompt us to bootstrap some files - choose only C sources. After that we will have our contrib files created:","title":"Creating initial files"},{"location":"create_extension/#initial-code","text":"Query execution pipeline has 3 stages: Parse/Semantic analysis - query string parsing and resolving tables Plan - query optimization and creating execution plan Execution - actual query execution Our logic will be added to the 2 stage, because we must check real execution plan, not Query. This is because after multiple transformations query can be changed in multiple ways - predicates can be deleted or added, therefore we may get a completely different query than in the original query string. To implement that we will create hook on planner - planner_hook . Inside we will invoke actual planner and check it's output for the existence of predicates. Starter code is the following: #include \"postgres.h\" #include \"fmgr.h\" #include \"optimizer/planner.h\" #ifdef PG_MODULE_MAGIC PG_MODULE_MAGIC; #endif static planner_hook_type prev_planner_hook; void _PG_init(void); void _PG_fini(void); static bool is_sus_query(Plan *plan) { /* ... */ return false; } static PlannedStmt * ban_sus_query_planner_hook(Query *parse, const char *query_string, int cursorOptions, ParamListInfo boundParams) { PlannedStmt *stmt; if (prev_planner_hook) stmt = prev_planner_hook(parse, query_string, cursorOptions, boundParams); else stmt = standard_planner(parse, query_string, cursorOptions, boundParams); if (is_sus_query(stmt->planTree)) ereport(ERROR, (errmsg(\"DML query does not contain predicates\"))); return stmt; } void _PG_init(void) { prev_planner_hook = planner_hook; planner_hook = ban_sus_query_planner_hook; } void _PG_fini(void) { planner_hook = prev_planner_hook; } Now we are ready to add \"business-logic\", but before let's understand how such suspicious queries look like.","title":"Initial code"},{"location":"create_extension/#examine-queries","text":"Suspicious query - is a DELETE/UPDATE query that does not contain predicates. One of the benefits that we are checking already planned statements is that all predicates are already optimized in a sense that boolean rules are applied. Query plan - is a tree of Plan nodes. Each Plan contains lefttree / righttree - left and right children and qual - list of predicates to apply at this node. But we must check only UPDATE/DELETE nodes, not each node, - nodes for them is ModifyTable . Thus our goal is: traverse query tree, find ModifyTable and check that it's qual is not empty But, before run sample queries to look what their queries looks like (inside) and which predicates they have. For tests we will use this setup: -- Schema CREATE TABLE tbl(x int); -- Test queries DELETE FROM tbl; DELETE FROM tbl WHERE x = 0; UPDATE tbl SET x = 1; UPDATE tbl SET x = 1 WHERE x = 0; To do this we will use our contrib - install it using make install , add to shared_preload_libraries='ban_sus_query' and put a breakpoint to return in ban_sus_query_planner_hook function. When we run first DELETE query without predicate, we will see the following: PlannedStmt contains top-level ModifyTable with empty qual list Inner SeqScan also contains empty qual list Now run DELETE query with predicate: PlannedStmt still contains empty qual list Inner SeqScan now contains qual with single element - equality predicate This is no surprise, because our ModifyTable does not apply any filtering - it just takes tuples from children (note, that by convention single-child nodes store them in lefttree ), so it's qual is empty, but filtering is applied to SeqScan - we must check this. As you can mention, extension shows all Node variables with actual types, without showing generic Plan entry. Also extension is able to show you elements of container types ( List * in this example). More than that, it renders Expr nodes (expressions) as it was in a query, so you do not have to manually check each field, trying to figure out what expression it is. In vanilla PostgreSQL you would have to evaluate 2 expressions: (first) get NodeTag and (second) cast variable to obtained NodeTag . In this example, to show stmt->planTree all you need to do is expand the tree node in variables explorer, but manually (without extension), you need to evaluate (i.e. in watch ) 2 expressions/steps: ((Node *)planTree)->type get T_ModifyTable - tag of ModifyTable node, and then (ModifyTable *)planTree - show variable with real type. Such manipulations take roughly 5 second, but, as this time accumulates, totally it can take up to 1 hour in a day - just to show variable's contents! But there is not such support for Expr variables - you will not see their representation. For this you have to dump variable to log using pprint function, which is not very convenient when you developing in IDE. Now we are ready to write some code.","title":"Examine queries"},{"location":"create_extension/#is_sus_query-implementation","text":"I repeat, our goal is to traverse query tree, find ModifyTable and check that it's qual is not empty , but now we can refine it: Search for ModifyTable in Plan tree and check that it's children have non-empty qual list As tree traversal is a recursive function, we will use 2 recursive functions: is_sus_query - main function that traverses plan tree to find ModifyTable node, and when it finds one invokes... contains_predicates - function that checks that this Plan node contains any predicate in a query Let's start with is_sus_query . All we have to do here is to check that Plan is a ModifyTable and if so, then check that it's children contain predicates. Node type checking is a frequent operation, so extension ships with some snippets - one of them is a isaif , which expands to if(IsA()) check: When we have determined, that it is a DML operation check that it is DELETE or UPDATE, because ModifyTable is used for other operations, i.e. INSERT . This is not hard - just check operation member. static bool is_sus_query(Plan *plan) { /* ... */ ModifyTable *modify = (ModifyTable *)plan; switch (modify->operation) { case CMD_UPDATE: case CMD_DELETE: /* Check predicates */ break; default: break; } /* ... */ } And now check these operations contain predicates using contains_predicates function (will be defined further). Also, do not forget to handle recursion: call is_sus_query for children and handle end case ( NULL ). The result function looks like this: static bool is_sus_query(Plan *plan) { /* Recursion end */ if (plan == NULL) return false; if (IsA(plan, ModifyTable)) { ModifyTable *modify = (ModifyTable *)plan; switch (modify->operation) { case CMD_UPDATE: case CMD_DELETE: return !contains_predicates(modify->plan.lefttree); default: break; } } /* Handle recursion */ return is_sus_query(plan->lefttree) || is_sus_query(plan->righttree); }","title":"is_sus_query implementation"},{"location":"create_extension/#contains_predicates-implementation","text":"Now perform actual checking of the predicates existence using contains_predicates . Inside this function we must check that given Plan contains predicates. But situation is complicated by the fact that only base Plan is given and we do not know actual query. For example this query: DELETE FROM t1 using t2 where t1.x = t2.x; Will contain JOIN in lefttree of ModifyTable : Thus we have to clarify what does contains_predicates must check. In order not to complicate things a lot, we will just find first node with any predicate. static bool contains_predicates(Plan *plan) { if (plan == NULL) return false; if (plan->qual != NIL) return true; return contains_predicates(plan->lefttree) || contains_predicates(plan->righttree); }","title":"contains_predicates implementation"},{"location":"create_extension/#testing","text":"First things first - test on example queries we defined above: postgres=# delete from tbl; ERROR: DML query does not contain predicates postgres=# delete from t1 where x = 0; DELETE 0 postgres=# update tbl set x = 0; ERROR: DML query does not contain predicates postgres=# update tbl set x = 0 where x = 0; UPDATE 0 It's working as expected. Also, as we injected our contrib as the last step, we can handle more complicated cases, like: postgres=# delete from t1 where true; ERROR: DML query does not contain predicates","title":"Testing"},{"location":"create_extension/#further-improvements","text":"This is just the beginning of the contrib, because there are lot's of corner cases that are not handled. For example, if we change true to false in last query, then we still will get an ERROR . That is because the database has realized that subquery will not return anything, so replaced with \"dummy\" Plan - Result node with FALSE one-time check, so nothing will be returned:","title":"Further improvements"},{"location":"create_extension/#result","text":"So far we have seen how you can quickly create new contrib using single command that will create all necessary files. To write some templated code, we used isaif snippet to quickly add check for Node type. Also, we have traversed query plan tree and saw it's nodes, without requirement to obtain NodeTag and cast to given type, which incredibly boosts performance. And like the icing on the cake we saw expression representations of predicates. For our purposes this is not a very big deal, because query contained only 1 predicate, but in large queries with dozens of different predicates it's just a lifesaver.","title":"Result"},{"location":"dev_scripts/","text":"Development scripts Most development operations can be effectively optimized using shell scripts. This page describes various scripts that can help in database development and debugging. During development/debugging you will be working with specific database version and installation, because you do not want to mess everything up with different versions. We will use the following architecture: ./ - project root with all source files ./build - binary installation directory: bin , share , lib , include ./data - directory with database installation ( PGDATA ) ./scripts - special development scripts (this section describes them) build.sh - running configure script and building database run.sh - managing database installation: create, start, stop, etc... clean.sh - cleaning files: database, build artifacts, etc... env.sh - generated file with exported environmental variables Each file contains it's own --help flag which show available options and usage examples. Scripts can be found in GitHub repository . build.sh In previous section we discussed how the database is compiled, so this script will do that - run configure and build database. But that's not all. Usually you will work with single server, no multi-server cluster. So it would be more convenient to just run pg_ctl start or initdb that will work with the specified installation or just type psql to quickly connect to server. So, after configure script has executed we will create env file, that will store PostgreSQL special environmental variables. Taking into an account default values for database/user/password/etc... we can generate env file in the following way: PGINSTDIR=\"$(pwd)/build\" ./configure --prefix=\"$PGINSTDIR\" # ... # shell env file SHENVFILE=\"$(pwd)/scripts/env.sh\" cat <<EOF >\"$SHENVFILE\" export PGINSTDIR=\"$PGINSTDIR\" export PGDATA=\"$(pwd)/data\" export PGHOST=\"localhost\" export PGPORT=\"5432\" export PGUSER=\"postgres\" export PGDATABASE=\"postgres\" export PATH=\"$PGINSTDIR/bin:\\$PATH\" LD_LIBRARY_PATH=\"\\${LD_LIBRARY_PATH:-''}\" export LD_LIBRARY_PATH=\"\\$PGINSTDIR/lib:\\$LD_LIBRARY_PATH\" EOF Using this file you can just source it and then use all binaries inside database installation path that will work with specific database installation. source ./scripts/env.sh # These commands use binaries under \"./build/bin\" # and work with database in \"./data\" initdb pg_ctl start run.sh There are 4 main operations with database: create start connect using psql stop Script run.sh will support all these operations and will be just a wrapper around: initdb , pg_ctl and psql . clean.sh As the name implies, this script is responsible for cleaning. It cleans: build artifacts installation files created database There is no magic in this file.","title":"Development scripts"},{"location":"dev_scripts/#development-scripts","text":"Most development operations can be effectively optimized using shell scripts. This page describes various scripts that can help in database development and debugging. During development/debugging you will be working with specific database version and installation, because you do not want to mess everything up with different versions. We will use the following architecture: ./ - project root with all source files ./build - binary installation directory: bin , share , lib , include ./data - directory with database installation ( PGDATA ) ./scripts - special development scripts (this section describes them) build.sh - running configure script and building database run.sh - managing database installation: create, start, stop, etc... clean.sh - cleaning files: database, build artifacts, etc... env.sh - generated file with exported environmental variables Each file contains it's own --help flag which show available options and usage examples. Scripts can be found in GitHub repository .","title":"Development scripts"},{"location":"dev_scripts/#buildsh","text":"In previous section we discussed how the database is compiled, so this script will do that - run configure and build database. But that's not all. Usually you will work with single server, no multi-server cluster. So it would be more convenient to just run pg_ctl start or initdb that will work with the specified installation or just type psql to quickly connect to server. So, after configure script has executed we will create env file, that will store PostgreSQL special environmental variables. Taking into an account default values for database/user/password/etc... we can generate env file in the following way: PGINSTDIR=\"$(pwd)/build\" ./configure --prefix=\"$PGINSTDIR\" # ... # shell env file SHENVFILE=\"$(pwd)/scripts/env.sh\" cat <<EOF >\"$SHENVFILE\" export PGINSTDIR=\"$PGINSTDIR\" export PGDATA=\"$(pwd)/data\" export PGHOST=\"localhost\" export PGPORT=\"5432\" export PGUSER=\"postgres\" export PGDATABASE=\"postgres\" export PATH=\"$PGINSTDIR/bin:\\$PATH\" LD_LIBRARY_PATH=\"\\${LD_LIBRARY_PATH:-''}\" export LD_LIBRARY_PATH=\"\\$PGINSTDIR/lib:\\$LD_LIBRARY_PATH\" EOF Using this file you can just source it and then use all binaries inside database installation path that will work with specific database installation. source ./scripts/env.sh # These commands use binaries under \"./build/bin\" # and work with database in \"./data\" initdb pg_ctl start","title":"build.sh"},{"location":"dev_scripts/#runsh","text":"There are 4 main operations with database: create start connect using psql stop Script run.sh will support all these operations and will be just a wrapper around: initdb , pg_ctl and psql .","title":"run.sh"},{"location":"dev_scripts/#cleansh","text":"As the name implies, this script is responsible for cleaning. It cleans: build artifacts installation files created database There is no magic in this file.","title":"clean.sh"},{"location":"postgresql_setup/","text":"PostgreSQL setup Building from source code PostgreSQL build is 2 staged: run configure script and then make . configure PostgreSQL uses autoconf to setup special pg_config.h header file. It contains lots of macros describing target environment, compiler capabilities, etc... To create this file you first run ./configure script and pass various flags. The set of flags differs between versions, but for development we can outline required: $ ./configure --prefix=$(pwd)/build \\ --enable-debug \\ --enable-cassert \\ --enable-tap-tests \\ --enable-depend \\ CFLAGS=\"-O0 -g3\" What we used: --prefix=$(pwd)/build - tells to install all binaries inside current working directory, thus you can safely work with multiple versions of PostgreSQL without disrupting the work of others --enable-debug - add debug symbols to result binary. This is required for debugging and variable exploring. --enable-cassert - enable Assert macros that check state consistency. Otherwise it is easy to violate internal state. --enable-tap-tests - enable using of TAP-tests using Perl. For this you may need to install Perl on your system. --enable-depend - enable tracking of changed files in your system, so you will rebuild only required files, without rebuilding full project. CFLAGS=\"-O0 -g3\" - tell the compiler to: -O0 - use lowest optimization level (so we can see all variables) -g3 - include as much debug symbols as we can Prefer using -g3 level if you are using PostgreSQL Hacker Helper extension, because it allows to use macro definitions and other features that can significantly improve debug experience. make The next step is actual building the sources. This is done much easier - just run make . To speed up compilation you can provide number of parallel threads by -j [THREADS] flag (or omit to use all available - make -j ). When the building is done run make install , so all binaries will be installed in build directory (directory that we specified as configure step).","title":"PostgreSQL setup"},{"location":"postgresql_setup/#postgresql-setup","text":"","title":"PostgreSQL setup"},{"location":"postgresql_setup/#building-from-source-code","text":"PostgreSQL build is 2 staged: run configure script and then make .","title":"Building from source code"},{"location":"postgresql_setup/#configure","text":"PostgreSQL uses autoconf to setup special pg_config.h header file. It contains lots of macros describing target environment, compiler capabilities, etc... To create this file you first run ./configure script and pass various flags. The set of flags differs between versions, but for development we can outline required: $ ./configure --prefix=$(pwd)/build \\ --enable-debug \\ --enable-cassert \\ --enable-tap-tests \\ --enable-depend \\ CFLAGS=\"-O0 -g3\" What we used: --prefix=$(pwd)/build - tells to install all binaries inside current working directory, thus you can safely work with multiple versions of PostgreSQL without disrupting the work of others --enable-debug - add debug symbols to result binary. This is required for debugging and variable exploring. --enable-cassert - enable Assert macros that check state consistency. Otherwise it is easy to violate internal state. --enable-tap-tests - enable using of TAP-tests using Perl. For this you may need to install Perl on your system. --enable-depend - enable tracking of changed files in your system, so you will rebuild only required files, without rebuilding full project. CFLAGS=\"-O0 -g3\" - tell the compiler to: -O0 - use lowest optimization level (so we can see all variables) -g3 - include as much debug symbols as we can Prefer using -g3 level if you are using PostgreSQL Hacker Helper extension, because it allows to use macro definitions and other features that can significantly improve debug experience.","title":"configure"},{"location":"postgresql_setup/#make","text":"The next step is actual building the sources. This is done much easier - just run make . To speed up compilation you can provide number of parallel threads by -j [THREADS] flag (or omit to use all available - make -j ). When the building is done run make install , so all binaries will be installed in build directory (directory that we specified as configure step).","title":"make"},{"location":"vscode_setup/","text":"Visual Studio Code setup Here is shown VS Code setup for PostgreSQL debugging. Extensions PostgreSQL Hacker Helper This is the main extension we are talking about. It significantly simplifies development and debugging of source code. Link to the extension . This is the only extension I recommend installing, because there are no alternatives to it. For the further extensions you are free to choose that suit you - no restrictions, just suggestions. Debugger extension First things first, you have to install debugger extension, which will provide debugging functionality. There are 2 supported (by PostgreSQL Hacker Helper) debugger extensions: C/C++ CodeLLDB Which one to choose is up to you, but I use a rule of thumb: if have built source code using gcc , then C/C++ with dbg debugger, otherwise ( clang ) use CodeLLDB with lldb debugger. Also, you would like to have autocompletions. You can use IntelliCode Completions . Perl PostgreSQL has different test types. One of them is TAP-tests which are written in Perl, so you might want to add extension with Perl support. Example, Perl extension. Markdown This is utility extension that will help create markdown files. They are popular because many documentation or README are written using Markdown syntax. Example: Markdown All in One . SQL queries SQL is the main language, so SQL-syntax support is must-have. You can use builtin SQL syntax support, or install Better PostgreSQL syntax extension which provides several PostgreSQL specific syntax features, like type cast. Database connections When developing you may need to connect to database and execute queries. For this you can choose any tool: psql , pgAdmin or vs code extension. There is no recommendation, because I do not use VS Code extension: only psql or pgAdmin , because VS Code extension targets primarily on Database usage while I am as database source code developer can request specific features that general extension does not provide. In example, I can create patch, that breaks binary protocol compatibility or adds extensions to it, which is obviously not supported by extension. Thus it is more preferable to have your own automation scripts. Moreover, you may have multiple different versions of PostgreSQL installed on your system simultaneously and again, it is unlikely that the general solution (extension) takes into account such features. launch.json Link to file on GitHub File .vscode/launch.json describes debug session configuration: name, debugger, path to binary/pid to attach, launch args, etc... When we are talking about PostgreSQL you should remember that it has multi-process architecture, not multi-threaded, this defines how we start debugging. Next, typical configurations for different uses cases will be presented. Backend Mostly you will be debugging a backend. It forks from postmaster, setup it's own state and then start main query processing loop. As it forks, then we can not just launch backend as usual binary - we have to attach to specific pid. { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Backend (cppdbg)\", \"type\": \"cppdbg\", \"request\": \"attach\", \"program\": \"${workspaceFolder}/src/backend/postgres\", \"processId\": \"${command:pickProcess}\", \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true } ], \"internalConsoleOptions\": \"neverOpen\", }, { \"name\": \"Backend (lldb)\", \"type\": \"lldb\", \"request\": \"attach\", \"program\": \"${workspaceFolder}/src/backend/postgres\", \"pid\": \"${command:pickProcess}\", \"internalConsoleOptions\": \"neverOpen\" } ] } These are template configurations created by default, but with some customization: To get PID of process special value is used: ${command:pickProcess} - it will open quick pick window where you can choose backend to attach. It shows all running processes, but actually all you have to do is to type \"postgres\" and choose penultimate element - usually it is the only running backend. \"program\" points to src/backend/postgres - default location of postgres binary. It contains all server debug symbols and it's location do not change, so you do not have to specify installation path each time. internalConsoleOptions is set to neverOpen because when debugging starts C/C++ extension opens Debug Console and shows logs, but usually it is not necessary and just only knocks down the focus. Frontend Frontend - are all utilities that run outside the server, i.e. pg_dump , pg_ctl , etc... They are separate binaries, so you can launch them directly, but usually they interact with the database, so they need database installation info. We can pass it directly using flags, but a better idea would be to use environment variables, because different binaries can use different flags. After installation all frontend utilities will be located in PGINSTDIR - directory specified during database setup, so all you have to do is just pick required binary in it. For example configuration for pg_ctl would be: { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"pg_ctl\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/build/bin/pg_ctl\", \"cwd\": \"${workspaceFolder}\", \"args\": [ \"status\" ], \"environment\": [ { \"name\": \"PGDATA\", \"value\": \"${workspaceFolder}/data\" } ] } ] } Here we are debugging pg_ctl status command (see \"args\" ) and pass PGDATA environment variable directly. The value of it can be any, but in the example I suppose that for development purposes your installation in data directory in the repository itself. A better idea than passing environment variables would be to pass environmental variable file . It have 2 benefits against manual specifying: This file can be automatically generated during database creation If you have configuration for multiple binaries, then you do not have to enter the same parameters - just pass this env file. On Development scripts page are shown useful development scripts, one of them ( build.sh ) will create special .env file, which contains all generated environment variables, so you can just specify this file: { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"pg_ctl\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/build/bin/pg_ctl\", \"cwd\": \"${workspaceFolder}\", \"args\": [ \"status\" ], \"envFile\": \"${workspaceFolder}/scripts/.env\" } ] } CoreDump Sometimes SEGFAULT happens or Assert test fails. In such cases a coredump will be created (you can enable them using ulimit -c unlimited shell command from root). C/C++ extension has special configuration for debugging CoreDump: { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"pg_ctl\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/src/backend/postgres\", \"cwd\": \"${workspaceFolder}\", \"coreDumpPath\": \"${input:coreDumpPath}\" } ], \"inputs\": [ { \"id\": \"coreDumpPath\", \"type\": \"promptString\", \"description\": \"Enter path to CoreDump\" } ] } To start debugging core dump you have to enter path to core dump file in prompt. PostgreSQL Hacker Helper also works with core dumps. tasks.json Link to file on GitHub Another useful functionality of VS Code are Tasks. In short, Task - is a named command, which can be any shell script. The schema of task entry is very simple: { \"label\": \"Name of task\", \"detail\": \"Description of a task\", \"command\": \"path/to/command\", \"args\": [ \"additional\", \"args\", \"to\", \"command\" ], } On Development scripts page we have defined some useful scripts and now it's time to integrate them into IDE. This file can be very large, so here we define the most common and necessary. Build To build we can use build.sh script. It accepts --build flag, that starts building. But it also accepts number of threads to use and we will use it. { \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"Build\", \"command\": \"${workspaceFolder}/scripts/build.sh\", \"args\": [ \"--build\", \"-j\", \"${input:threads}\" ], \"detail\": \"Run build and install DB with all contribs\", \"group\": { \"kind\": \"build\", \"isDefault\": true } } ], \"inputs\": [ { \"id\": \"threads\", \"type\": \"pickString\", \"options\": [ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\" ], \"default\": \"8\", \"description\": \"Number of threads to use\" } ] } As you can see this task runs our build.sh script and passes number of threads to it. And number of threads is prompted interactively using quick-pick - you can see example on screenshot below. More than that, we also added a \"group\" member. What did it give us? Now we can run build just by using shortcut Ctrl + Shift + B . And that's it - build is running! Running PSQL Next step after compilation is a database creation and connecting to it. For this task we have run.sh script which manages database instance. It may seem annoying to constantly run the same task sequence: Init database -> Run database -> Run PSQL . So we will combine all 3 commands into single task. { \"label\": \"Run psql\", \"detail\": \"Run psql\", \"command\": \"${workspaceFolder}/scripts/run.sh\", \"args\": [ \"--init-db\", \"--run-db\", \"--psql\" ], \"isBackground\": true, } Now just after database compilation has completed you just types psql in Run task prompt and clicks required task. That's it - you are running PSQL and can send queries! Start terminal in database environment Complex tasks are not that simple, so sometimes you have to do everything manually. For such kind of work you must start a new terminal session using env.sh script defined earlier. It will setup environment variables in such way, so you will be working with compiled binaries and database instance. { \"label\": \"Run terminal\", \"detail\": \"Run terminal with imported environment variables specific to environment\", \"presentation\": { \"echo\": true, \"reveal\": \"always\", \"focus\": true, \"panel\": \"shared\", \"showReuseMessage\": false, \"clear\": false }, \"command\": \"/usr/bin/sh\", \"args\": [ \"-c\", \". ${workspaceFolder}/scripts/env.sh; $SHELL\" ], \"isBackground\": true }, After launching this task a new shell with all PG env variables will be created. Other scripts In tasks.json file you can find more predefined tasks for VS Code. They include: Running tests (using Ctrl + Shift + T shortcut) Managing database instance Cleaning files","title":"VS Code setup"},{"location":"vscode_setup/#visual-studio-code-setup","text":"Here is shown VS Code setup for PostgreSQL debugging.","title":"Visual Studio Code setup"},{"location":"vscode_setup/#extensions","text":"","title":"Extensions"},{"location":"vscode_setup/#postgresql-hacker-helper","text":"This is the main extension we are talking about. It significantly simplifies development and debugging of source code. Link to the extension . This is the only extension I recommend installing, because there are no alternatives to it. For the further extensions you are free to choose that suit you - no restrictions, just suggestions.","title":"PostgreSQL Hacker Helper"},{"location":"vscode_setup/#debugger-extension","text":"First things first, you have to install debugger extension, which will provide debugging functionality. There are 2 supported (by PostgreSQL Hacker Helper) debugger extensions: C/C++ CodeLLDB Which one to choose is up to you, but I use a rule of thumb: if have built source code using gcc , then C/C++ with dbg debugger, otherwise ( clang ) use CodeLLDB with lldb debugger. Also, you would like to have autocompletions. You can use IntelliCode Completions .","title":"Debugger extension"},{"location":"vscode_setup/#perl","text":"PostgreSQL has different test types. One of them is TAP-tests which are written in Perl, so you might want to add extension with Perl support. Example, Perl extension.","title":"Perl"},{"location":"vscode_setup/#markdown","text":"This is utility extension that will help create markdown files. They are popular because many documentation or README are written using Markdown syntax. Example: Markdown All in One .","title":"Markdown"},{"location":"vscode_setup/#sql-queries","text":"SQL is the main language, so SQL-syntax support is must-have. You can use builtin SQL syntax support, or install Better PostgreSQL syntax extension which provides several PostgreSQL specific syntax features, like type cast.","title":"SQL queries"},{"location":"vscode_setup/#database-connections","text":"When developing you may need to connect to database and execute queries. For this you can choose any tool: psql , pgAdmin or vs code extension. There is no recommendation, because I do not use VS Code extension: only psql or pgAdmin , because VS Code extension targets primarily on Database usage while I am as database source code developer can request specific features that general extension does not provide. In example, I can create patch, that breaks binary protocol compatibility or adds extensions to it, which is obviously not supported by extension. Thus it is more preferable to have your own automation scripts. Moreover, you may have multiple different versions of PostgreSQL installed on your system simultaneously and again, it is unlikely that the general solution (extension) takes into account such features.","title":"Database connections"},{"location":"vscode_setup/#launchjson","text":"Link to file on GitHub File .vscode/launch.json describes debug session configuration: name, debugger, path to binary/pid to attach, launch args, etc... When we are talking about PostgreSQL you should remember that it has multi-process architecture, not multi-threaded, this defines how we start debugging. Next, typical configurations for different uses cases will be presented.","title":"launch.json"},{"location":"vscode_setup/#backend","text":"Mostly you will be debugging a backend. It forks from postmaster, setup it's own state and then start main query processing loop. As it forks, then we can not just launch backend as usual binary - we have to attach to specific pid. { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Backend (cppdbg)\", \"type\": \"cppdbg\", \"request\": \"attach\", \"program\": \"${workspaceFolder}/src/backend/postgres\", \"processId\": \"${command:pickProcess}\", \"MIMode\": \"gdb\", \"setupCommands\": [ { \"description\": \"Enable pretty-printing for gdb\", \"text\": \"-enable-pretty-printing\", \"ignoreFailures\": true } ], \"internalConsoleOptions\": \"neverOpen\", }, { \"name\": \"Backend (lldb)\", \"type\": \"lldb\", \"request\": \"attach\", \"program\": \"${workspaceFolder}/src/backend/postgres\", \"pid\": \"${command:pickProcess}\", \"internalConsoleOptions\": \"neverOpen\" } ] } These are template configurations created by default, but with some customization: To get PID of process special value is used: ${command:pickProcess} - it will open quick pick window where you can choose backend to attach. It shows all running processes, but actually all you have to do is to type \"postgres\" and choose penultimate element - usually it is the only running backend. \"program\" points to src/backend/postgres - default location of postgres binary. It contains all server debug symbols and it's location do not change, so you do not have to specify installation path each time. internalConsoleOptions is set to neverOpen because when debugging starts C/C++ extension opens Debug Console and shows logs, but usually it is not necessary and just only knocks down the focus.","title":"Backend"},{"location":"vscode_setup/#frontend","text":"Frontend - are all utilities that run outside the server, i.e. pg_dump , pg_ctl , etc... They are separate binaries, so you can launch them directly, but usually they interact with the database, so they need database installation info. We can pass it directly using flags, but a better idea would be to use environment variables, because different binaries can use different flags. After installation all frontend utilities will be located in PGINSTDIR - directory specified during database setup, so all you have to do is just pick required binary in it. For example configuration for pg_ctl would be: { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"pg_ctl\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/build/bin/pg_ctl\", \"cwd\": \"${workspaceFolder}\", \"args\": [ \"status\" ], \"environment\": [ { \"name\": \"PGDATA\", \"value\": \"${workspaceFolder}/data\" } ] } ] } Here we are debugging pg_ctl status command (see \"args\" ) and pass PGDATA environment variable directly. The value of it can be any, but in the example I suppose that for development purposes your installation in data directory in the repository itself. A better idea than passing environment variables would be to pass environmental variable file . It have 2 benefits against manual specifying: This file can be automatically generated during database creation If you have configuration for multiple binaries, then you do not have to enter the same parameters - just pass this env file. On Development scripts page are shown useful development scripts, one of them ( build.sh ) will create special .env file, which contains all generated environment variables, so you can just specify this file: { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"pg_ctl\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/build/bin/pg_ctl\", \"cwd\": \"${workspaceFolder}\", \"args\": [ \"status\" ], \"envFile\": \"${workspaceFolder}/scripts/.env\" } ] }","title":"Frontend"},{"location":"vscode_setup/#coredump","text":"Sometimes SEGFAULT happens or Assert test fails. In such cases a coredump will be created (you can enable them using ulimit -c unlimited shell command from root). C/C++ extension has special configuration for debugging CoreDump: { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"pg_ctl\", \"type\": \"cppdbg\", \"request\": \"launch\", \"program\": \"${workspaceFolder}/src/backend/postgres\", \"cwd\": \"${workspaceFolder}\", \"coreDumpPath\": \"${input:coreDumpPath}\" } ], \"inputs\": [ { \"id\": \"coreDumpPath\", \"type\": \"promptString\", \"description\": \"Enter path to CoreDump\" } ] } To start debugging core dump you have to enter path to core dump file in prompt. PostgreSQL Hacker Helper also works with core dumps.","title":"CoreDump"},{"location":"vscode_setup/#tasksjson","text":"Link to file on GitHub Another useful functionality of VS Code are Tasks. In short, Task - is a named command, which can be any shell script. The schema of task entry is very simple: { \"label\": \"Name of task\", \"detail\": \"Description of a task\", \"command\": \"path/to/command\", \"args\": [ \"additional\", \"args\", \"to\", \"command\" ], } On Development scripts page we have defined some useful scripts and now it's time to integrate them into IDE. This file can be very large, so here we define the most common and necessary.","title":"tasks.json"},{"location":"vscode_setup/#build","text":"To build we can use build.sh script. It accepts --build flag, that starts building. But it also accepts number of threads to use and we will use it. { \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"Build\", \"command\": \"${workspaceFolder}/scripts/build.sh\", \"args\": [ \"--build\", \"-j\", \"${input:threads}\" ], \"detail\": \"Run build and install DB with all contribs\", \"group\": { \"kind\": \"build\", \"isDefault\": true } } ], \"inputs\": [ { \"id\": \"threads\", \"type\": \"pickString\", \"options\": [ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\" ], \"default\": \"8\", \"description\": \"Number of threads to use\" } ] } As you can see this task runs our build.sh script and passes number of threads to it. And number of threads is prompted interactively using quick-pick - you can see example on screenshot below. More than that, we also added a \"group\" member. What did it give us? Now we can run build just by using shortcut Ctrl + Shift + B . And that's it - build is running!","title":"Build"},{"location":"vscode_setup/#running-psql","text":"Next step after compilation is a database creation and connecting to it. For this task we have run.sh script which manages database instance. It may seem annoying to constantly run the same task sequence: Init database -> Run database -> Run PSQL . So we will combine all 3 commands into single task. { \"label\": \"Run psql\", \"detail\": \"Run psql\", \"command\": \"${workspaceFolder}/scripts/run.sh\", \"args\": [ \"--init-db\", \"--run-db\", \"--psql\" ], \"isBackground\": true, } Now just after database compilation has completed you just types psql in Run task prompt and clicks required task. That's it - you are running PSQL and can send queries!","title":"Running PSQL"},{"location":"vscode_setup/#start-terminal-in-database-environment","text":"Complex tasks are not that simple, so sometimes you have to do everything manually. For such kind of work you must start a new terminal session using env.sh script defined earlier. It will setup environment variables in such way, so you will be working with compiled binaries and database instance. { \"label\": \"Run terminal\", \"detail\": \"Run terminal with imported environment variables specific to environment\", \"presentation\": { \"echo\": true, \"reveal\": \"always\", \"focus\": true, \"panel\": \"shared\", \"showReuseMessage\": false, \"clear\": false }, \"command\": \"/usr/bin/sh\", \"args\": [ \"-c\", \". ${workspaceFolder}/scripts/env.sh; $SHELL\" ], \"isBackground\": true }, After launching this task a new shell with all PG env variables will be created.","title":"Start terminal in database environment"},{"location":"vscode_setup/#other-scripts","text":"In tasks.json file you can find more predefined tasks for VS Code. They include: Running tests (using Ctrl + Shift + T shortcut) Managing database instance Cleaning files","title":"Other scripts"}]}